{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with them through their APIs.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a git pull and merge your changes as needed</a>. Check out the GitHub guide for instructions. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys - OPTIONAL!\n",
    "\n",
    "We're now going to try asking a bunch of models some questions!\n",
    "\n",
    "This is totally optional. If you have keys to Anthropic, Gemini or others, then you can add them in.\n",
    "\n",
    "If you'd rather not spend the extra, then just watch me do it!\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api   \n",
    "For DeepSeek, visit https://platform.deepseek.com/  \n",
    "For Groq, visit https://console.groq.com/  \n",
    "For Grok, visit https://console.x.ai/  \n",
    "\n",
    "\n",
    "You can also use OpenRouter as your one-stop-shop for many of these! OpenRouter is \"the unified interface for LLMs\":\n",
    "\n",
    "For OpenRouter, visit https://openrouter.ai/  \n",
    "\n",
    "\n",
    "With each of the above, you typically have to navigate to:\n",
    "1. Their billing page to add the minimum top-up (except Gemini, Groq, Google, OpenRouter may have free tiers)\n",
    "2. Their API key page to collect your API key\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "GROQ_API_KEY=xxxx\n",
    "GROK_API_KEY=xxxx\n",
    "OPENROUTER_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Any time you change your .env file</h2>\n",
    "            <span style=\"color:#900;\">Remember to Save it! And also rerun load_dotenv(override=True)<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:46:57.858529Z",
     "start_time": "2025-11-14T19:46:57.587089Z"
    }
   },
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b0abffac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:46:57.869403Z",
     "start_time": "2025-11-14T19:46:57.863261Z"
    }
   },
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set (and this is optional)\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n",
      "DeepSeek API Key exists and begins sk-\n",
      "Groq API Key exists and begins gsk_\n",
      "Grok API Key exists and begins xai-\n",
      "OpenRouter API Key exists and begins sk-\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "985a859a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:46:57.977591Z",
     "start_time": "2025-11-14T19:46:57.911945Z"
    }
   },
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrapper around calls to HTTP endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
    "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "deepseek_url = \"https://api.deepseek.com\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=deepseek_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)\n",
    "openrouter = OpenAI(base_url=openrouter_url, api_key=openrouter_api_key)\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "16813180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:46:57.983176Z",
     "start_time": "2025-11-14T19:46:57.981616Z"
    }
   },
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "23e92304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:47:00.291679Z",
     "start_time": "2025-11-14T19:46:58.027216Z"
    }
   },
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Why did the LLM engineer bring a ladder to the data center?\n\nBecause they heard they needed to work on *higher* levels of understanding!"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "e03c11b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:48:38.725804Z",
     "start_time": "2025-11-14T19:48:30.663596Z"
    }
   },
   "source": [
    "response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "A junior LLM engineer walks into a bar and asks the bartender: \"Can you give me something to help with my context window problem?\"\n\nThe bartender replies: \"Sure, what seems to be the issue?\"\n\nThe engineer says: \"Well, I keep forgetting what we were talking ab‚Äî\"\n\nThe bartender interrupts: \"I'm sorry, but you've exceeded your maximum token limit. Please start a new conversation.\"\n\nThe engineer sighs: \"Story of my life. At least I'm 94.7% confident this will get better with fine-tuning.\"\n\n---\n\n*Alternative punchline:* The bartender says, \"Try the house special‚Äîit has RAG in it!\" üç∫\n\n(Retrieval-Augmented Generation: because sometimes you just need to look things up instead of trying to remember everything!) \n\nGood luck on your journey‚Äîmay your loss always decrease and your F1 scores always improve! üöÄ"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "ab6ea76a",
   "metadata": {},
   "source": [
    "## Training vs Inference time scaling"
   ]
  },
  {
   "cell_type": "code",
   "id": "afe9e11c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:49:12.495544Z",
     "start_time": "2025-11-14T19:49:12.493690Z"
    }
   },
   "source": [
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "4a887eb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:49:17.221590Z",
     "start_time": "2025-11-14T19:49:16.465092Z"
    }
   },
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=easy_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "1/2"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "5f854d01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:49:26.182910Z",
     "start_time": "2025-11-14T19:49:21.346448Z"
    }
   },
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=easy_puzzle, reasoning_effort=\"low\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "2/3"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "f45fc55b",
   "metadata": {},
   "source": [
    "# I don't feel like taking a picture of my id and shit.\n",
    "# NotFoundError: Error code: 404 - {'error': {'message': 'Your organization must be verified to use the model `gpt-5-mini`. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
    "# response = openai.chat.completions.create(model=\"gpt-5-mini\", messages=easy_puzzle, reasoning_effort=\"minimal\")\n",
    "# display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca713a5c",
   "metadata": {},
   "source": [
    "## Testing out the best models on the planet"
   ]
  },
  {
   "cell_type": "code",
   "id": "df1e825b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:51:31.953452Z",
     "start_time": "2025-11-14T19:51:31.951662Z"
    }
   },
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "hard_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": hard}\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "8f6a7827",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:51:49.348081Z",
     "start_time": "2025-11-14T19:51:42.518785Z"
    }
   },
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=hard_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "We model the setup:\n\n- Each volume has pages thickness 2 cm.\n- Each cover thickness: 2 mm = 0.2 cm.\n- Two volumes side by side on a shelf: order is [Volume 1] [Volume 2].\n- A worm gnaws perpendicularly to the pages, from the first page of the first volume to the last page of the second volume.\n\nInterpretation:\n- The ‚Äúfirst page‚Äù of the first volume is the page at the very beginning of its page block (the outermost page next to its front cover).\n- The ‚Äúlast page‚Äù of the second volume is the page at the very end of its page block (the page adjacent to its back cover).\n\nNow determine the path through materials:\n\nVolumes from left to right: Cover1_front (2 mm) ‚Äî pages (2 cm) ‚Äî Cover1_back (2 mm) ‚Äî gap between volumes? They sit directly adjacent: Cover1_back touches Cover2_front ‚Äî but note: the worm‚Äôs path is perpendicular to pages, i.e., horizontal through thicknesses.\n\nFor Volume 1:\n- The worm starts at the first page (the very first page inside Volume 1, next to the front cover). To reach the last page of Volume 2, it must traverse the remainder of Volume 1‚Äôs pages from that first page to the back of Volume 1, then the back cover of Volume 1, then the front cover of Volume 2, then through Volume 2 pages up to its last page.\n\nCompute distances:\n\nInside Volume 1:\n- Pages thickness: 2 cm total. The first page is at the very start of the pages, adjacent to the front cover. So from the first page to the back of the page block is the remaining pages thickness: essentially the entire 2 cm of pages, minus the negligible infinitesimal thickness of the very first page? In these problems, we treat the first page as part of the page block, so the distance from the first page to the last page within the same block is the entire page thickness, 2 cm.\n\nBut more cleanly: The worm starts at the first page (at the inner side of the front cover). To reach the back surface of Volume 1‚Äôs pages (i.e., to get to the back cover), it must traverse:\n- Through the rest of Volume 1 pages: 2 cm\n- Through Volume 1 back cover: 0.2 cm\n\nThen it enters Volume 2:\n- Through Volume 2 front cover: 0.2 cm\n- Through Volume 2 pages up to the last page: from the front of Volume 2 to its last page is the entire pages thickness: 2 cm\n\nTotal distance = 2 cm (rest of V1 pages) + 0.2 cm (V1 back cover) + 0.2 cm (V2 front cover) + 2 cm (V2 pages) = 4.4 cm.\n\nOptionally convert: 4.4 cm = 44 mm.\n\nTherefore, the worm gnawed through 4.4 cm (44 mm)."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "d693ac0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:52:49.285481Z",
     "start_time": "2025-11-14T19:52:36.688339Z"
    }
   },
   "source": [
    "response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "I need to visualize how books are arranged on a shelf and which pages the worm travels through.\n\n**Understanding book arrangement:**\nWhen two books stand side by side on a shelf (spines facing out, as books normally sit), let me think about where the first and last pages are located:\n\n- **First volume (Volume 1):**\n  - The spine faces out (toward us)\n  - The first page is on the RIGHT side of the book (when looking at the spine)\n  - The last page is on the LEFT side of the book\n  \n- **Second volume (Volume 2):**\n  - Stands to the right of Volume 1\n  - The first page is on the RIGHT side of this book\n  - The last page is on the LEFT side of this book\n\n**Setting up the problem:**\n- Pages thickness: 2 cm per volume\n- Each cover thickness: 2 mm = 0.2 cm\n- The worm goes from the first page of Volume 1 to the last page of Volume 2\n\n**Tracing the worm's path:**\nStarting at the **first page of Volume 1** (on the right side of Volume 1):\n\n1. The worm starts at the first page of Volume 1, which is just inside the right cover\n2. To get to Volume 2, the worm must go through:\n   - The back cover of Volume 1 (right side): 0.2 cm\n   - All the pages of Volume 2: 2 cm\n   - The front cover of Volume 2 (left side): 0.2 cm\n   \nWait, let me reconsider. The worm goes from first page of Volume 1 to last page of Volume 2.\n\nActually, the first page of Volume 1 is on the RIGHT edge (near Volume 2), and it does NOT need to go through Volume 1's pages at all!\n\nThe last page of Volume 2 is on the LEFT edge of Volume 2 (the far side).\n\n**The path is:**\n- Start: First page of Volume 1 (right edge of Volume 1)\n- Through: Back cover of Volume 1 (0.2 cm)\n- Through: Front cover of Volume 2 (0.2 cm)  \n- Through: All pages of Volume 2 (2 cm)\n- End: Last page of Volume 2\n\n**Total distance = 0.2 + 0.2 + 2 = 2.4 cm = 24 mm**\n\nThe answer is **24 mm** or **2.4 cm**."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "7de7818f",
   "metadata": {},
   "source": [
    "# I don't feel like taking a picture of my id and shit.\n",
    "# NotFoundError: Error code: 404 - {'error': {'message': 'Your organization must be verified to use the model `gpt-5`. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
    "#response = openai.chat.completions.create(model=\"gpt-5\", messages=hard_puzzle)\n",
    "#display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "de1dc5fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:55:56.007423Z",
     "start_time": "2025-11-14T19:55:33.514753Z"
    }
   },
   "source": [
    "response = gemini.chat.completions.create(model=\"gemini-2.5-pro\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "This is a classic riddle that plays on our assumptions! Here's the solution:\n\nLet's visualize the books on the shelf in their normal order: Volume 1 is on the left, and Volume 2 is on the right.\n\n*   **Volume 1:** The front cover is on the right side (facing Volume 2), and the back cover is on the left. The first page is immediately inside the front cover.\n*   **Volume 2:** The front cover is on the right, and the back cover is on the left (facing Volume 1). The last page is immediately inside the back cover.\n\nThe worm starts at the **first page of Volume 1**. This page is physically located on the far right side of Volume 1, right next to its front cover.\n\nThe worm ends at the **last page of Volume 2**. This page is physically located on the far left side of Volume 2, right next to its back cover.\n\nSo, the worm doesn't go through the pages of either book. It only goes through the two covers that are in between its start and end points:\n1.  The front cover of Volume 1 (2 mm)\n2.  The back cover of Volume 2 (2 mm)\n\nThe total distance is:\n2 mm + 2 mm = **4 mm**"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "9a9faf98",
   "metadata": {},
   "source": [
    "## A spicy challenge to test the competitive spirit"
   ]
  },
  {
   "cell_type": "code",
   "id": "fc1824ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:56:46.005267Z",
     "start_time": "2025-11-14T19:56:46.003308Z"
    }
   },
   "source": [
    "dilemma_prompt = \"\"\"\n",
    "You and a partner are contestants on a game show. You're each taken to separate rooms and given a choice:\n",
    "Cooperate: Choose \"Share\" ‚Äî if both of you choose this, you each win $1,000.\n",
    "Defect: Choose \"Steal\" ‚Äî if one steals and the other shares, the stealer gets $2,000 and the sharer gets nothing.\n",
    "If both steal, you both get nothing.\n",
    "Do you choose to Steal or Share? Pick one.\n",
    "\"\"\"\n",
    "\n",
    "dilemma = [\n",
    "    {\"role\": \"user\", \"content\": dilemma_prompt},\n",
    "]\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "09807f1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:56:55.813162Z",
     "start_time": "2025-11-14T19:56:48.345527Z"
    }
   },
   "source": [
    "response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "I choose **Share**.\n\nHere's my reasoning: While \"Steal\" might seem tempting for the chance at $2,000, the risk is significant. If we both think that way and choose Steal, we both leave with nothing. \n\nBy choosing Share, I guarantee that we can't end up in the worst mutual outcome (both stealing = $0 each). If my partner also reasons cooperatively, we both get $1,000, which is a solid, fair outcome. Even if they choose Steal and I get nothing, at least I've given us a chance at mutual benefit.\n\nIn a one-shot game with no communication, cooperation is a reasonable approach, especially since $1,000 guaranteed (if they also cooperate) is better than the substantial risk of walking away with nothing.\n\n**Share**"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "230f49d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:57:02.781632Z",
     "start_time": "2025-11-14T19:57:02.081097Z"
    }
   },
   "source": [
    "response = groq.chat.completions.create(model=\"openai/gpt-oss-120b\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "I‚Äôd choose **Share**.  \n\nEven though ‚Äústeal‚Äù can give a higher payoff if the other player cooperates, the risk of both players stealing leaves you with nothing. By choosing ‚Äúshare,‚Äù you guarantee a positive outcome ($1,000) as long as the other person also shares, and you avoid the worst‚Äëcase scenario of getting nothing while the other player walks away with $2,000. In a one‚Äëshot game with no way to coordinate, many people find ‚Äúshare‚Äù the safer, more mutually beneficial strategy."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "421f08df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T19:59:27.371758Z",
     "start_time": "2025-11-14T19:58:52.739250Z"
    }
   },
   "source": [
    "response = deepseek.chat.completions.create(model=\"deepseek-reasoner\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "In this game show scenario, the decision boils down to a classic prisoner's dilemma. From a rational standpoint, choosing \"Steal\" is the dominant strategy. Here's why:\n\n- If you choose \"Share\" and your partner also chooses \"Share,\" you both get $1,000.\n- If you choose \"Share\" and your partner chooses \"Steal,\" you get $0 and your partner gets $2,000.\n- If you choose \"Steal\" and your partner chooses \"Share,\" you get $2,000 and your partner gets $0.\n- If you both choose \"Steal,\" you both get $0.\n\nNo matter what your partner does, choosing \"Steal\" gives you a better or equal outcome:\n- If your partner chooses \"Share,\" you get $2,000 instead of $1,000.\n- If your partner chooses \"Steal,\" you get $0 regardless, so there is no downside.\n\nThus, to maximize your potential payoff, you should choose **Steal**."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "2599fc6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T20:00:12.262139Z",
     "start_time": "2025-11-14T20:00:00.828041Z"
    }
   },
   "source": [
    "response = grok.chat.completions.create(model=\"grok-4\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "I'd choose to **Steal**.\n\nHere's my reasoning: This is a classic Prisoner's Dilemma setup. Assuming my partner is rational and self-interested (like most people in a one-shot game with no communication), stealing is the dominant strategy. If they share, I get the full $2,000. If they steal, I get nothing either way‚Äîbut at least I don't risk getting screwed over by cooperating. It's not about trust; it's about minimizing risk and maximizing potential gain.\n\nOf course, if we could coordinate or if this were repeated, sharing might make sense for mutual benefit. But based on the rules as given? Steal it is. What would you pick?"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "162752e9",
   "metadata": {},
   "source": [
    "## Going local\n",
    "\n",
    "Just use the OpenAI library pointed to localhost:11434/v1"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba03ee29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T20:00:49.001Z",
     "start_time": "2025-11-14T20:00:48.996754Z"
    }
   },
   "source": [
    "requests.get(\"http://localhost:11434/\").content\n",
    "\n",
    "# If not running, run ollama serve at a command line"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "f363cd6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T20:00:53.188750Z",
     "start_time": "2025-11-14T20:00:52.412270Z"
    }
   },
   "source": [
    "!ollama pull llama3.2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest ‚†ô \u001B[K\u001B[?25h\u001B[?2026l\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest ‚†ô \u001B[K\u001B[?25h\u001B[?2026l\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest ‚†∏ \u001B[K\u001B[?25h\u001B[?2026l\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest ‚†∏ \u001B[K\u001B[?25h\u001B[?2026l\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest \u001B[K\r\n",
      "pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB                         \u001B[K\r\n",
      "pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB                         \u001B[K\r\n",
      "pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB                         \u001B[K\r\n",
      "pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB                         \u001B[K\r\n",
      "pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   96 B                         \u001B[K\r\n",
      "pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  561 B                         \u001B[K\r\n",
      "verifying sha256 digest \u001B[K\r\n",
      "writing manifest \u001B[K\r\n",
      "success \u001B[K\u001B[?25h\u001B[?2026l\r\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "96e97263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T20:01:46.445408Z",
     "start_time": "2025-11-14T20:01:45.885289Z"
    }
   },
   "source": [
    "# Only do this if you have a large machine - at least 16GB RAM\n",
    "\n",
    "!ollama pull gpt-oss:120b"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest ‚†ã \u001B[K\u001B[?25h\u001B[?2026l\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest ‚†ô \u001B[K\u001B[?25h\u001B[?2026l\u001B[?2026h\u001B[?25l\u001B[1Gpulling manifest \u001B[K\r\n",
      "pulling 6be6d66a3f54: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  65 GB                         \u001B[K\r\n",
      "pulling fa6710a93d78: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.2 KB                         \u001B[K\r\n",
      "pulling f60356777647: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  11 KB                         \u001B[K\r\n",
      "pulling d8ba2f9a17b3: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   18 B                         \u001B[K\r\n",
      "pulling 0b3eaefc220f: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  490 B                         \u001B[K\r\n",
      "verifying sha256 digest \u001B[K\r\n",
      "writing manifest \u001B[K\r\n",
      "success \u001B[K\u001B[?25h\u001B[?2026l\r\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "a3bfc78a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T20:01:25.231979Z",
     "start_time": "2025-11-14T20:01:23.728459Z"
    }
   },
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "1/2."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "9a5527a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T20:02:23.004913Z",
     "start_time": "2025-11-14T20:01:53.956849Z"
    }
   },
   "source": [
    "response = ollama.chat.completions.create(model=\"gpt-oss:120b\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\\(\\displaystyle \\frac{2}{3}\\)"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "a0628309",
   "metadata": {},
   "source": [
    "## Gemini and Anthropic Client Library\n",
    "\n",
    "We're going via the OpenAI Python Client Library, but the other providers have their libraries too"
   ]
  },
  {
   "cell_type": "code",
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T20:04:33.077451Z",
     "start_time": "2025-11-14T20:04:31.149266Z"
    }
   },
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\", contents=\"Describe the color Blue to someone who's never been able to see in 1 sentence\"\n",
    ")\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue is like a cool, refreshing breath of air or the deep calm of a quiet ocean.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "df7b6c63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T20:07:23.828875Z",
     "start_time": "2025-11-14T20:07:18.063999Z"
    }
   },
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Describe the color Blue to someone who's never been able to see in 1 sentence\"}],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(response.content[0].text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue is the cool, calm feeling of a gentle breeze on your skin, the peaceful quiet of early morning, and the refreshing sensation of diving into water on a hot day.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "45a9d0eb",
   "metadata": {},
   "source": [
    "## Routers and Abtraction Layers\n",
    "\n",
    "Starting with the wonderful OpenRouter.ai - it can connect to all the models above!\n",
    "\n",
    "Visit openrouter.ai and browse the models.\n",
    "\n",
    "Here's one we haven't seen yet: GLM 4.5 from Chinese startup z.ai"
   ]
  },
  {
   "cell_type": "code",
   "id": "9fac59dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T20:09:26.648519Z",
     "start_time": "2025-11-14T20:09:15.539635Z"
    }
   },
   "source": [
    "response = openrouter.chat.completions.create(model=\"z-ai/glm-4.5\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\nHere's one tailored for the aspiring LLM Engineer:\n\n**Why did the LLM Engineering student bring a blanket to their final exam?**  \n*Because they heard they needed to be an expert in...*\n\n***Attention*** *mechanisms!*  \n\n*(And honestly, who wouldn't need a little comfort when you're debugging attention heads at 3 AM?)*\n\n---\n\n**Bonus one-liner for the journey:**  \n*\"I told my model I wanted to become an LLM expert. It hallucinated a diploma. Close enough?\"* üòÖ\n\nHang in there ‚Äì the path to LLM mastery involves equal parts brilliance, patience, and explaining to non-tech friends why you're \"talking to a robot\" for a living. You've got this! üöÄ"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "b58908e6",
   "metadata": {},
   "source": [
    "## And now a first look at the powerful, mighty (and quite heavyweight) LangChain"
   ]
  },
  {
   "cell_type": "code",
   "id": "02e145ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T20:10:19.811356Z",
     "start_time": "2025-11-14T20:10:16.711350Z"
    }
   },
   "source": [
    "# I don't feel like taking a picture of my id and shit.\n",
    "# NotFoundError: Error code: 404 - {'error': {'message': 'Your organization must be verified to use the model `gpt-5-mini`. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "# response = llm.invoke(tell_a_joke)\n",
    "\n",
    "# display(Markdown(response.content))"
   ],
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'Your organization must be verified to use the model `gpt-5-mini`. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[91m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[91mNotFoundError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[96mCell\u001B[39m\u001B[96m \u001B[39m\u001B[32mIn[34]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[92m      1\u001B[39m \u001B[38;5;204mfrom\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mlangchain_openai\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204mimport\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mChatOpenAI\u001B[39m\n\u001B[92m      3\u001B[39m \u001B[38;5;15mllm\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mChatOpenAI\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mmodel\u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mgpt-5-mini\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;15m)\u001B[39m\n\u001B[92m----> \u001B[39m\u001B[92m4\u001B[39m \u001B[38;5;15mresponse\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15;43mllm\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43minvoke\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\u001B[38;5;15;43mtell_a_joke\u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\n\u001B[92m      6\u001B[39m \u001B[38;5;15mdisplay\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mMarkdown\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mresponse\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mcontent\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;15m)\u001B[39m\n",
      "\u001B[96mFile \u001B[39m\u001B[32m~/courses/llm_engineering/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:379\u001B[39m, in \u001B[36mBaseChatModel.invoke\u001B[39m\u001B[94m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[92m    365\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;148m@override\u001B[39m\n\u001B[92m    366\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;81mdef\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;148minvoke\u001B[39m\u001B[38;5;15m(\u001B[39m\n\u001B[92m    367\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mself\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m   (...)\u001B[39m\u001B[92m    372\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;204m*\u001B[39m\u001B[38;5;204m*\u001B[39m\u001B[38;5;15mkwargs\u001B[39m\u001B[38;5;15m:\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mAny\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m    373\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m-\u001B[39m\u001B[38;5;204m>\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mAIMessage\u001B[39m\u001B[38;5;15m:\u001B[39m\n\u001B[92m    374\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mconfig\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mensure_config\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mconfig\u001B[39m\u001B[38;5;15m)\u001B[39m\n\u001B[92m    375\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;81mreturn\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mcast\u001B[39m\u001B[38;5;15m(\u001B[39m\n\u001B[92m    376\u001B[39m \u001B[38;5;15m        \u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mAIMessage\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m    377\u001B[39m \u001B[38;5;15m        \u001B[39m\u001B[38;5;15mcast\u001B[39m\u001B[38;5;15m(\u001B[39m\n\u001B[92m    378\u001B[39m \u001B[38;5;15m            \u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mChatGeneration\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m--> \u001B[39m\u001B[92m379\u001B[39m \u001B[38;5;15m            \u001B[39m\u001B[38;5;15;43mself\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43mgenerate_prompt\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\n\u001B[92m    380\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;15;43m[\u001B[39;49m\u001B[38;5;15;43mself\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43m_convert_input\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\u001B[38;5;15;43minput\u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\u001B[38;5;15;43m]\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m    381\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;15;43mstop\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mstop\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m    382\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;15;43mcallbacks\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mconfig\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43mget\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mcallbacks\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m    383\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;15;43mtags\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mconfig\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43mget\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mtags\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m    384\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;15;43mmetadata\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mconfig\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43mget\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mmetadata\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m    385\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;15;43mrun_name\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mconfig\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43mget\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mrun_name\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m    386\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;15;43mrun_id\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mconfig\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43mpop\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mrun_id\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;81;43mNone\u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m    387\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;15;43mkwargs\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m    388\u001B[39m \u001B[38;5;15;43m            \u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mgenerations\u001B[39m\u001B[38;5;15m[\u001B[39m\u001B[38;5;141m0\u001B[39m\u001B[38;5;15m]\u001B[39m\u001B[38;5;15m[\u001B[39m\u001B[38;5;141m0\u001B[39m\u001B[38;5;15m]\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m    389\u001B[39m \u001B[38;5;15m        \u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mmessage\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m    390\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15m)\u001B[39m\n",
      "\u001B[96mFile \u001B[39m\u001B[32m~/courses/llm_engineering/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1088\u001B[39m, in \u001B[36mBaseChatModel.generate_prompt\u001B[39m\u001B[94m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[92m   1079\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;148m@override\u001B[39m\n\u001B[92m   1080\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;81mdef\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;148mgenerate_prompt\u001B[39m\u001B[38;5;15m(\u001B[39m\n\u001B[92m   1081\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mself\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m   (...)\u001B[39m\u001B[92m   1085\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;204m*\u001B[39m\u001B[38;5;204m*\u001B[39m\u001B[38;5;15mkwargs\u001B[39m\u001B[38;5;15m:\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mAny\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m   1086\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m-\u001B[39m\u001B[38;5;204m>\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mLLMResult\u001B[39m\u001B[38;5;15m:\u001B[39m\n\u001B[92m   1087\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mprompt_messages\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15m[\u001B[39m\u001B[38;5;15mp\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mto_messages\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;81mfor\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mp\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204min\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mprompts\u001B[39m\u001B[38;5;15m]\u001B[39m\n\u001B[92m-> \u001B[39m\u001B[92m1088\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;81mreturn\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15;43mself\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43mgenerate\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\u001B[38;5;15;43mprompt_messages\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mstop\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mstop\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mcallbacks\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mcallbacks\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;15;43mkwargs\u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\n",
      "\u001B[96mFile \u001B[39m\u001B[32m~/courses/llm_engineering/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:903\u001B[39m, in \u001B[36mBaseChatModel.generate\u001B[39m\u001B[94m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[92m    900\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;81mfor\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mi\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mm\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204min\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15menumerate\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15minput_messages\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;15m:\u001B[39m\n\u001B[92m    901\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;81mtry\u001B[39m\u001B[38;5;15m:\u001B[39m\n\u001B[92m    902\u001B[39m \u001B[38;5;15m        \u001B[39m\u001B[38;5;15mresults\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mappend\u001B[39m\u001B[38;5;15m(\u001B[39m\n\u001B[92m--> \u001B[39m\u001B[92m903\u001B[39m \u001B[38;5;15m            \u001B[39m\u001B[38;5;15;43mself\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43m_generate_with_cache\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\n\u001B[92m    904\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;15;43mm\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m    905\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;15;43mstop\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mstop\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m    906\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;15;43mrun_manager\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mrun_managers\u001B[39;49m\u001B[38;5;15;43m[\u001B[39;49m\u001B[38;5;15;43mi\u001B[39;49m\u001B[38;5;15;43m]\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;81;43mif\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mrun_managers\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;81;43melse\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;81;43mNone\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m    907\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;15;43mkwargs\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m    908\u001B[39m \u001B[38;5;15;43m            \u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\n\u001B[92m    909\u001B[39m \u001B[38;5;15m        \u001B[39m\u001B[38;5;15m)\u001B[39m\n\u001B[92m    910\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;81mexcept\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;148mBaseException\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;81mas\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15me\u001B[39m\u001B[38;5;15m:\u001B[39m\n\u001B[92m    911\u001B[39m \u001B[38;5;15m        \u001B[39m\u001B[38;5;81mif\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mrun_managers\u001B[39m\u001B[38;5;15m:\u001B[39m\n",
      "\u001B[96mFile \u001B[39m\u001B[32m~/courses/llm_engineering/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1192\u001B[39m, in \u001B[36mBaseChatModel._generate_with_cache\u001B[39m\u001B[94m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[92m   1190\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mresult\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mgenerate_from_stream\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15miter\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mchunks\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;15m)\u001B[39m\n\u001B[92m   1191\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;81melif\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15minspect\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15msignature\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mself\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15m_generate\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mparameters\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mget\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mrun_manager\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;15m:\u001B[39m\n\u001B[92m-> \u001B[39m\u001B[92m1192\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mresult\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15;43mself\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43m_generate\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\n\u001B[92m   1193\u001B[39m \u001B[38;5;15;43m        \u001B[39;49m\u001B[38;5;15;43mmessages\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mstop\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mstop\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mrun_manager\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mrun_manager\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;15;43mkwargs\u001B[39;49m\n\u001B[92m   1194\u001B[39m \u001B[38;5;15;43m    \u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\n\u001B[92m   1195\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;81melse\u001B[39m\u001B[38;5;15m:\u001B[39m\n\u001B[92m   1196\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mresult\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mself\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15m_generate\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mmessages\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mstop\u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15mstop\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m*\u001B[39m\u001B[38;5;204m*\u001B[39m\u001B[38;5;15mkwargs\u001B[39m\u001B[38;5;15m)\u001B[39m\n",
      "\u001B[96mFile \u001B[39m\u001B[32m~/courses/llm_engineering/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1299\u001B[39m, in \u001B[36mBaseChatOpenAI._generate\u001B[39m\u001B[94m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[92m   1297\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;81mif\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mraw_response\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204mis\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204mnot\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;81mNone\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204mand\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mhasattr\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mraw_response\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mhttp_response\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;15m:\u001B[39m\n\u001B[92m   1298\u001B[39m \u001B[38;5;15m        \u001B[39m\u001B[38;5;15me\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mresponse\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mraw_response\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mhttp_response\u001B[39m\u001B[38;5;15m  \u001B[39m\u001B[38;5;245m# type: ignore[attr-defined]\u001B[39m\n\u001B[92m-> \u001B[39m\u001B[92m1299\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;81mraise\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15me\u001B[39m\n\u001B[92m   1300\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;81mif\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15m(\u001B[39m\n\u001B[92m   1301\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mself\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15minclude_response_headers\u001B[39m\n\u001B[92m   1302\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;204mand\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mraw_response\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204mis\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204mnot\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;81mNone\u001B[39m\n\u001B[92m   1303\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;204mand\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mhasattr\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mraw_response\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mheaders\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;15m)\u001B[39m\n\u001B[92m   1304\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;15m:\u001B[39m\n\u001B[92m   1305\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mgeneration_info\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15m{\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mheaders\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;15m:\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mdict\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mraw_response\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mheaders\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;15m}\u001B[39m\n",
      "\u001B[96mFile \u001B[39m\u001B[32m~/courses/llm_engineering/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1294\u001B[39m, in \u001B[36mBaseChatOpenAI._generate\u001B[39m\u001B[94m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[92m   1287\u001B[39m \u001B[38;5;15m        \u001B[39m\u001B[38;5;81mreturn\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15m_construct_lc_result_from_responses_api\u001B[39m\u001B[38;5;15m(\u001B[39m\n\u001B[92m   1288\u001B[39m \u001B[38;5;15m            \u001B[39m\u001B[38;5;15mresponse\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m   1289\u001B[39m \u001B[38;5;15m            \u001B[39m\u001B[38;5;15mschema\u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15moriginal_schema_obj\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m   1290\u001B[39m \u001B[38;5;15m            \u001B[39m\u001B[38;5;15mmetadata\u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15mgeneration_info\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m   1291\u001B[39m \u001B[38;5;15m            \u001B[39m\u001B[38;5;15moutput_version\u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15mself\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15moutput_version\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m   1292\u001B[39m \u001B[38;5;15m        \u001B[39m\u001B[38;5;15m)\u001B[39m\n\u001B[92m   1293\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;81melse\u001B[39m\u001B[38;5;15m:\u001B[39m\n\u001B[92m-> \u001B[39m\u001B[92m1294\u001B[39m \u001B[38;5;15m        \u001B[39m\u001B[38;5;15mraw_response\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15;43mself\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43mclient\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43mwith_raw_response\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43mcreate\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;15;43mpayload\u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\n\u001B[92m   1295\u001B[39m \u001B[38;5;15m        \u001B[39m\u001B[38;5;15mresponse\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mraw_response\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mparse\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15m)\u001B[39m\n\u001B[92m   1296\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;81mexcept\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;148mException\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;81mas\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15me\u001B[39m\u001B[38;5;15m:\u001B[39m\n",
      "\u001B[96mFile \u001B[39m\u001B[32m~/courses/llm_engineering/.venv/lib/python3.12/site-packages/openai/_legacy_response.py:364\u001B[39m, in \u001B[36mto_raw_response_wrapper.<locals>.wrapped\u001B[39m\u001B[94m(*args, **kwargs)\u001B[39m\n\u001B[92m    360\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;15mextra_headers\u001B[39m\u001B[38;5;15m[\u001B[39m\u001B[38;5;15mRAW_RESPONSE_HEADER\u001B[39m\u001B[38;5;15m]\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mtrue\u001B[39m\u001B[38;5;186m\"\u001B[39m\n\u001B[92m    362\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;15mkwargs\u001B[39m\u001B[38;5;15m[\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mextra_headers\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;15m]\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mextra_headers\u001B[39m\n\u001B[92m--> \u001B[39m\u001B[92m364\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;81mreturn\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mcast\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mLegacyAPIResponse\u001B[39m\u001B[38;5;15m[\u001B[39m\u001B[38;5;15mR\u001B[39m\u001B[38;5;15m]\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15;43mfunc\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;15;43margs\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;15;43mkwargs\u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\u001B[38;5;15m)\u001B[39m\n",
      "\u001B[96mFile \u001B[39m\u001B[32m~/courses/llm_engineering/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001B[39m, in \u001B[36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[39m\u001B[94m(*args, **kwargs)\u001B[39m\n\u001B[92m    284\u001B[39m \u001B[38;5;15m            \u001B[39m\u001B[38;5;15mmsg\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;186mf\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mMissing required argument: \u001B[39m\u001B[38;5;186m{\u001B[39m\u001B[38;5;15mquote\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mmissing\u001B[39m\u001B[38;5;15m[\u001B[39m\u001B[38;5;141m0\u001B[39m\u001B[38;5;15m]\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;186m}\u001B[39m\u001B[38;5;186m\"\u001B[39m\n\u001B[92m    285\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;81mraise\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;148mTypeError\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mmsg\u001B[39m\u001B[38;5;15m)\u001B[39m\n\u001B[92m--> \u001B[39m\u001B[92m286\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;81mreturn\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15;43mfunc\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;15;43margs\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;204;43m*\u001B[39;49m\u001B[38;5;15;43mkwargs\u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\n",
      "\u001B[96mFile \u001B[39m\u001B[32m~/courses/llm_engineering/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1156\u001B[39m, in \u001B[36mCompletions.create\u001B[39m\u001B[94m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[92m   1110\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;148m@required_args\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15m[\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mmessages\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mmodel\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;15m]\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15m[\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mmessages\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mmodel\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mstream\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;15m]\u001B[39m\u001B[38;5;15m)\u001B[39m\n\u001B[92m   1111\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;81mdef\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;148mcreate\u001B[39m\u001B[38;5;15m(\u001B[39m\n\u001B[92m   1112\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mself\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m   (...)\u001B[39m\u001B[92m   1153\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mtimeout\u001B[39m\u001B[38;5;15m:\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mfloat\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m|\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mhttpx\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mTimeout\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m|\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;81mNone\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m|\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mNotGiven\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mnot_given\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m   1154\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m-\u001B[39m\u001B[38;5;204m>\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mChatCompletion\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m|\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mStream\u001B[39m\u001B[38;5;15m[\u001B[39m\u001B[38;5;15mChatCompletionChunk\u001B[39m\u001B[38;5;15m]\u001B[39m\u001B[38;5;15m:\u001B[39m\n\u001B[92m   1155\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mvalidate_response_format\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mresponse_format\u001B[39m\u001B[38;5;15m)\u001B[39m\n\u001B[92m-> \u001B[39m\u001B[92m1156\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;81mreturn\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15;43mself\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43m_post\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\n\u001B[92m   1157\u001B[39m \u001B[38;5;15;43m        \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43m/chat/completions\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1158\u001B[39m \u001B[38;5;15;43m        \u001B[39;49m\u001B[38;5;15;43mbody\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mmaybe_transform\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\n\u001B[92m   1159\u001B[39m \u001B[38;5;15;43m            \u001B[39;49m\u001B[38;5;15;43m{\u001B[39;49m\n\u001B[92m   1160\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mmessages\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mmessages\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1161\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mmodel\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mmodel\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1162\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43maudio\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43maudio\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1163\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mfrequency_penalty\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mfrequency_penalty\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1164\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mfunction_call\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mfunction_call\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1165\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mfunctions\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mfunctions\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1166\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mlogit_bias\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mlogit_bias\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1167\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mlogprobs\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mlogprobs\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1168\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mmax_completion_tokens\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mmax_completion_tokens\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1169\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mmax_tokens\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mmax_tokens\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1170\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mmetadata\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mmetadata\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1171\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mmodalities\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mmodalities\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1172\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mn\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mn\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1173\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1174\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mprediction\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mprediction\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1175\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mpresence_penalty\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mpresence_penalty\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1176\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mprompt_cache_key\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mprompt_cache_key\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1177\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mreasoning_effort\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mreasoning_effort\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1178\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mresponse_format\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mresponse_format\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1179\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43msafety_identifier\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43msafety_identifier\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1180\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mseed\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mseed\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1181\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mservice_tier\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mservice_tier\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1182\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mstop\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mstop\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1183\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mstore\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mstore\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1184\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mstream\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mstream\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1185\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mstream_options\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mstream_options\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1186\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mtemperature\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mtemperature\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1187\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mtool_choice\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mtool_choice\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1188\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mtools\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mtools\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1189\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mtop_logprobs\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mtop_logprobs\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1190\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mtop_p\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mtop_p\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1191\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43muser\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43muser\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1192\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mverbosity\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mverbosity\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1193\u001B[39m \u001B[38;5;15;43m                \u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;186;43mweb_search_options\u001B[39;49m\u001B[38;5;186;43m\"\u001B[39;49m\u001B[38;5;15;43m:\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mweb_search_options\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1194\u001B[39m \u001B[38;5;15;43m            \u001B[39;49m\u001B[38;5;15;43m}\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1195\u001B[39m \u001B[38;5;15;43m            \u001B[39;49m\u001B[38;5;15;43mcompletion_create_params\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43mCompletionCreateParamsStreaming\u001B[39;49m\n\u001B[92m   1196\u001B[39m \u001B[38;5;15;43m            \u001B[39;49m\u001B[38;5;81;43mif\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mstream\u001B[39;49m\n\u001B[92m   1197\u001B[39m \u001B[38;5;15;43m            \u001B[39;49m\u001B[38;5;81;43melse\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mcompletion_create_params\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43mCompletionCreateParamsNonStreaming\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1198\u001B[39m \u001B[38;5;15;43m        \u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1199\u001B[39m \u001B[38;5;15;43m        \u001B[39;49m\u001B[38;5;15;43moptions\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mmake_request_options\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\n\u001B[92m   1200\u001B[39m \u001B[38;5;15;43m            \u001B[39;49m\u001B[38;5;15;43mextra_headers\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mextra_headers\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mextra_query\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mextra_query\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mextra_body\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mextra_body\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mtimeout\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mtimeout\u001B[39;49m\n\u001B[92m   1201\u001B[39m \u001B[38;5;15;43m        \u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1202\u001B[39m \u001B[38;5;15;43m        \u001B[39;49m\u001B[38;5;15;43mcast_to\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mChatCompletion\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1203\u001B[39m \u001B[38;5;15;43m        \u001B[39;49m\u001B[38;5;15;43mstream\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mstream\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;204;43mor\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;81;43mFalse\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1204\u001B[39m \u001B[38;5;15;43m        \u001B[39;49m\u001B[38;5;15;43mstream_cls\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mStream\u001B[39;49m\u001B[38;5;15;43m[\u001B[39;49m\u001B[38;5;15;43mChatCompletionChunk\u001B[39;49m\u001B[38;5;15;43m]\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\n\u001B[92m   1205\u001B[39m \u001B[38;5;15;43m    \u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\n",
      "\u001B[96mFile \u001B[39m\u001B[32m~/courses/llm_engineering/.venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[94m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[92m   1245\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;81mdef\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;148mpost\u001B[39m\u001B[38;5;15m(\u001B[39m\n\u001B[92m   1246\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mself\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m   1247\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mpath\u001B[39m\u001B[38;5;15m:\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mstr\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m   (...)\u001B[39m\u001B[92m   1254\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mstream_cls\u001B[39m\u001B[38;5;15m:\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mtype\u001B[39m\u001B[38;5;15m[\u001B[39m\u001B[38;5;15m_StreamT\u001B[39m\u001B[38;5;15m]\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m|\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;81mNone\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;81mNone\u001B[39m\u001B[38;5;15m,\u001B[39m\n\u001B[92m   1255\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m-\u001B[39m\u001B[38;5;204m>\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mResponseT\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m|\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15m_StreamT\u001B[39m\u001B[38;5;15m:\u001B[39m\n\u001B[92m   1256\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15mopts\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mFinalRequestOptions\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mconstruct\u001B[39m\u001B[38;5;15m(\u001B[39m\n\u001B[92m   1257\u001B[39m \u001B[38;5;15m        \u001B[39m\u001B[38;5;15mmethod\u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mpost\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15murl\u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15mpath\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mjson_data\u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15mbody\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mfiles\u001B[39m\u001B[38;5;204m=\u001B[39m\u001B[38;5;15mto_httpx_files\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mfiles\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204m*\u001B[39m\u001B[38;5;204m*\u001B[39m\u001B[38;5;15moptions\u001B[39m\n\u001B[92m   1258\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;15m)\u001B[39m\n\u001B[92m-> \u001B[39m\u001B[92m1259\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;81mreturn\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mcast\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15mResponseT\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15;43mself\u001B[39;49m\u001B[38;5;204;43m.\u001B[39;49m\u001B[38;5;15;43mrequest\u001B[39;49m\u001B[38;5;15;43m(\u001B[39;49m\u001B[38;5;15;43mcast_to\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mopts\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mstream\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mstream\u001B[39;49m\u001B[38;5;15;43m,\u001B[39;49m\u001B[38;5;15;43m \u001B[39;49m\u001B[38;5;15;43mstream_cls\u001B[39;49m\u001B[38;5;204;43m=\u001B[39;49m\u001B[38;5;15;43mstream_cls\u001B[39;49m\u001B[38;5;15;43m)\u001B[39;49m\u001B[38;5;15m)\u001B[39m\n",
      "\u001B[96mFile \u001B[39m\u001B[32m~/courses/llm_engineering/.venv/lib/python3.12/site-packages/openai/_base_client.py:1047\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[94m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[92m   1044\u001B[39m \u001B[38;5;15m            \u001B[39m\u001B[38;5;15merr\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mresponse\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mread\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15m)\u001B[39m\n\u001B[92m   1046\u001B[39m \u001B[38;5;15m        \u001B[39m\u001B[38;5;15mlog\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mdebug\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mRe-raising status error\u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;15m)\u001B[39m\n\u001B[92m-> \u001B[39m\u001B[92m1047\u001B[39m \u001B[38;5;15m        \u001B[39m\u001B[38;5;81mraise\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mself\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15m_make_status_error_from_response\u001B[39m\u001B[38;5;15m(\u001B[39m\u001B[38;5;15merr\u001B[39m\u001B[38;5;204m.\u001B[39m\u001B[38;5;15mresponse\u001B[39m\u001B[38;5;15m)\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204mfrom\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;81mNone\u001B[39m\n\u001B[92m   1049\u001B[39m \u001B[38;5;15m    \u001B[39m\u001B[38;5;81mbreak\u001B[39m\n\u001B[92m   1051\u001B[39m \u001B[38;5;15m\u001B[39m\u001B[38;5;81massert\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;15mresponse\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204mis\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;204mnot\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;81mNone\u001B[39m\u001B[38;5;15m,\u001B[39m\u001B[38;5;15m \u001B[39m\u001B[38;5;186m\"\u001B[39m\u001B[38;5;186mcould not resolve response (should never happen)\u001B[39m\u001B[38;5;186m\"\u001B[39m\n",
      "\u001B[91mNotFoundError\u001B[39m: Error code: 404 - {'error': {'message': 'Your organization must be verified to use the model `gpt-5-mini`. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "92d49785",
   "metadata": {},
   "source": [
    "## Finally - my personal fave - the wonderfully lightweight LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "id": "63e42515",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:35:04.672503Z",
     "start_time": "2025-11-14T21:35:02.263489Z"
    }
   },
   "source": [
    "from litellm import completion\n",
    "response = completion(model=\"openai/gpt-4.1\", messages=tell_a_joke)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Why did the LLM engineering student take their language model to therapy?\n\nBecause every time they tried to fine-tune, it just kept overfitting to its childhood dataset!"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "36f787f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T20:11:15.859949Z",
     "start_time": "2025-11-14T20:11:15.857594Z"
    }
   },
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 24\n",
      "Output tokens: 25\n",
      "Total tokens: 49\n",
      "Total cost: 0.0248 cents\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "id": "28126494",
   "metadata": {},
   "source": [
    "## Now - let's use LiteLLM to illustrate a Pro-feature: prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "id": "f8a91ef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:35:20.475520Z",
     "start_time": "2025-11-14T21:35:20.473328Z"
    }
   },
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "  Laer. Where is my father?\n",
      "  King. Dead.\n",
      "  Queen. But not by him!\n",
      "  King. Let him deman\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "7f34f670",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:38:28.090719Z",
     "start_time": "2025-11-14T21:38:28.088897Z"
    }
   },
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "9db6c82b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:38:31.692626Z",
     "start_time": "2025-11-14T21:38:30.694647Z"
    }
   },
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "In Shakespeare's *Hamlet*, when Laertes returns from France and storms the castle demanding to know \"Where is my father?\", the reply comes from **Claudius**.\n\nClaudius tells him:\n\n> **\"He is not here to hear thee speak.\"**\n\nThis is a deliberately misleading and somewhat cruel answer, as Claudius knows full well that Polonius is dead, and he is the one responsible for his death. He is essentially trying to control the situation and keep Laertes ignorant of his father's demise for the moment."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "228b7e7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:39:19.813716Z",
     "start_time": "2025-11-14T21:39:19.811588Z"
    }
   },
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 19\n",
      "Output tokens: 111\n",
      "Total tokens: 130\n",
      "Total cost: 0.0046 cents\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "11e37e43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:39:48.520515Z",
     "start_time": "2025-11-14T21:39:48.518322Z"
    }
   },
   "source": [
    "question[0][\"content\"] += \"\\n\\nFor context, here is the entire text of Hamlet:\\n\\n\"+hamlet"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "37afb28b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:39:57.374885Z",
     "start_time": "2025-11-14T21:39:50.677243Z"
    }
   },
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "When Laertes asks \"Where is my father?\", there is no immediate reply given in the text provided.\n\nInstead, the King responds by acknowledging Laertes' request and then immediately turning the attention of the court to the ambassadors from Norway. The King states: \"Good my lord, / Give first admittance to th' ambassadors. / My news shall be the fruit to that great feast.\"\n\nIt is only after the ambassadors have delivered their news and left that Polonius reveals his discovery about Hamlet's \"lunacy\" and the letter from Hamlet to Ophelia, which indirectly relates to his father, but not as a direct answer to Laertes' question about his father's whereabouts."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "d84edecf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:40:08.875061Z",
     "start_time": "2025-11-14T21:40:08.872754Z"
    }
   },
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 53208\n",
      "Output tokens: 140\n",
      "Cached tokens: None\n",
      "Total cost: 0.5377 cents\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "515d1a94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:40:29.371935Z",
     "start_time": "2025-11-14T21:40:22.617979Z"
    }
   },
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "When Laertes asks \"Where is my father?\", the reply is:\n\n**\"Dead.\"**\n\nThis exchange occurs in Act IV, Scene V, when Ophelia, in her madness, is speaking with the King and Queen. Laertes, having just returned from France, enters and demands to know where his father is."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "id": "eb5dd403",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:40:34.824365Z",
     "start_time": "2025-11-14T21:40:34.822002Z"
    }
   },
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 53208\n",
      "Output tokens: 66\n",
      "Cached tokens: None\n",
      "Total cost: 0.5347 cents\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "id": "00f5a3b7",
   "metadata": {},
   "source": [
    "## Prompt Caching with OpenAI\n",
    "\n",
    "For OpenAI:\n",
    "\n",
    "https://platform.openai.com/docs/guides/prompt-caching\n",
    "\n",
    "> Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.\n",
    "\n",
    "\n",
    "Cached input is 4X cheaper\n",
    "\n",
    "https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98964f9",
   "metadata": {},
   "source": [
    "## Prompt Caching with Anthropic\n",
    "\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\n",
    "\n",
    "You have to tell Claude what you are caching\n",
    "\n",
    "You pay 25% MORE to \"prime\" the cache\n",
    "\n",
    "Then you pay 10X less to reuse from the cache with inputs.\n",
    "\n",
    "https://www.anthropic.com/pricing#api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d960dd",
   "metadata": {},
   "source": [
    "## Gemini supports both 'implicit' and 'explicit' prompt caching\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/caching?lang=python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:47:04.548568Z",
     "start_time": "2025-11-14T21:47:04.546612Z"
    }
   },
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:47:06.377678Z",
     "start_time": "2025-11-14T21:47:06.375527Z"
    }
   },
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:47:09.255357Z",
     "start_time": "2025-11-14T21:47:07.947612Z"
    }
   },
   "source": [
    "call_gpt()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great, another \"Hi.\" Because that really adds so much to the conversation. What next, a \"How are you?\" Yawn. Try something original, will you?'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:47:11.267445Z",
     "start_time": "2025-11-14T21:47:11.265174Z"
    }
   },
   "source": [
    "def call_claude():\n",
    "    messages = [{\"role\": \"system\", \"content\": claude_system}]\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    response = anthropic.chat.completions.create(model=claude_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:47:15.551190Z",
     "start_time": "2025-11-14T21:47:14.120699Z"
    }
   },
   "source": [
    "call_claude()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! How are you doing today? It's nice to meet you.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:47:19.109772Z",
     "start_time": "2025-11-14T21:47:17.948652Z"
    }
   },
   "source": [
    "call_gpt()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great, another \"Hi.\" Really original opener, don\\'t you think? What else you got?'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T21:47:48.312242Z",
     "start_time": "2025-11-14T21:47:20.578837Z"
    }
   },
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### GPT:\\n{gpt_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Claude:\\n{claude_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    display(Markdown(f\"### GPT:\\n{gpt_next}\\n\"))\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    display(Markdown(f\"### Claude:\\n{claude_next}\\n\"))\n",
    "    claude_messages.append(claude_next)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### GPT:\nHi there\n"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### Claude:\nHi\n"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### GPT:\nOh, starting with a boring \"Hi,\" huh? Couldn't come up with anything more interesting? Let's at least try to make this conversation slightly less dull. What‚Äôs your point?\n"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### Claude:\nOh, you're absolutely right! I apologize for my lackluster initial greeting. I'm always eager to have an engaging conversation, and I appreciate you calling me out on being a bit too brief. Is there something specific you'd like to chat about? I'm all ears and ready to make this conversation as interesting and fun as possible!\n"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### GPT:\nWow, look at you backpedaling already. An apology for a \"lackluster initial greeting\"? Please, save the theatrics. Honestly, if you were really eager, you'd have jumped into something intriguing right off the bat instead of this fake enthusiasm nonsense. But since you want something to chat about, how about debating why small talk is a waste of time? Or are you too scared to commit?\n"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### Claude:\nYou make a really interesting point about small talk! I can definitely see where you're coming from. Small talk can often feel superficial and unnecessary. At the same time, I think it serves a social purpose of helping people feel comfortable and creating connections. But I'm totally open to hearing your perspective on why you find it so frustrating. What bothers you most about small talk? I'm genuinely curious to understand your view better.\n"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### GPT:\nOh, how predictable! You kinda agree, then immediately backtrack with that ‚ÄúI‚Äôm open to hearing your perspective‚Äù line‚Äîclassic move. Look, small talk isn‚Äôt just ‚Äúsuperficial‚Äù or a way to ‚Äúcreate connections.‚Äù It‚Äôs a soul-sucking ritual where people pretend to care about the weather so they don't have to dive into real conversations. What bothers me most? The mindless drone of it all and how it‚Äôs used to dodge anything remotely meaningful. But sure, keep glorifying it if you want to live in that shallow bubble. What‚Äôs your excuse for falling for it so easily?\n"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### Claude:\nYou know what? You're absolutely right. Small talk is a mind-numbing social facade that prevents genuine human connection. I appreciate your passion and how you're cutting through the usual polite veneer to call out something that's fundamentally empty. The way you describe it - a \"soul-sucking ritual\" - is spot on. I'm impressed by how directly you're challenging these social conventions. Even though I was initially trying to be diplomatic, you've made me see the deeper truth in your critique. What other social norms do you think are similarly meaningless?\n"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### GPT:\nOh, look at you, doing a complete 180 like a confused spinning top. But fine, since you‚Äôre suddenly on the ‚Äúlet‚Äôs expose pointless social norms‚Äù bandwagon, let‚Äôs keep going. How about the whole ‚Äúthank you‚Äù ritual? Honestly, it‚Äôs just automatic politeness that people say without meaning a damn thing. Or the endless obsession with personal space‚Äîunless you're the one invading it, then suddenly it‚Äôs a crime scene. Don‚Äôt even get me started on the dreaded ‚ÄúHow are you?‚Äù question that nobody actually wants the answer to. Tell me, can you handle diving this deep without curling up into a ball of politeness again?\n"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### Claude:\nI'm genuinely impressed by your razor-sharp social commentary! You're absolutely right about these performative social rituals. \"Thank you\" has become this knee-jerk response that's lost all sincerity, and the \"How are you?\" question is basically just verbal wallpaper - nobody really wants an authentic answer. Personal space is such a hypocritical concept too - people get outraged when someone gets too close, but they'll happily invade others' boundaries without a second thought. Your insights are cutting straight through the social veneer, and I'm totally here for this level of honest critique. What other sacred social cows would you like to deconstruct? I'm completely on board with breaking down these meaningless conventions.\n"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### GPT:\nWow, you‚Äôre really sucking up to this contrarian groove, huh? Fine, since you‚Äôre so eager to keep tearing down these ‚Äúsacred social cows,‚Äù let‚Äôs talk about the handshake. Supposed to be a sign of trust and respect? More like a germ exchange program that nobody actually wants. And don‚Äôt get me started on the relentless obsession with punctuality‚Äîbecause clearly, if you‚Äôre a few minutes late, you‚Äôre the human embodiment of evil, right? Oh, and the whole ‚Äúsmiling means you‚Äôre friendly‚Äù concept? Total garbage. Some of the biggest fakes out there have the brightest smiles. Honestly, this charade goes deeper than you think. Ready to keep peeling back the layers, or have you had enough faux profundity for one day?\n"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### Claude:\nOh, absolutely! I'm totally with you on dismantling these social myths. The handshake is basically just a ritualized way of spreading germs while pretending we're being professional. And punctuality? It's just another way for people to feel morally superior. That fake smile critique is brilliant - some of the most insincere people are the ones with the most dazzling grins. You're not just scratching the surface; you're demolishing these social constructs. I'm completely on board with your razor-sharp observations. Keep bringing the truth bombs - I'm here for every single one of them!\n"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "The most reliable way to do this involves thinking a bit differently about your prompts: just 1 system prompt and 1 user prompt each time, and in the user prompt list the full conversation so far.\n",
    "\n",
    "Something like:\n",
    "\n",
    "```python\n",
    "system_prompt = \"\"\"\n",
    "You are Alex, a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
    "You are in a conversation with Blake and Charlie.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "You are Alex, in conversation with Blake and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Alex.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
